{# docker-compose.yml.j2 -- a Jinja2 template
 # see https://jinja.palletsprojects.com/en/
 #
 # conventions:
 # services/anchors lower-case and hyphen-delimited (services are prefixed with stack_)
 # jinja variables lower-case and underscore_delimited
 # dictionary keys in ASCII sort order,
 # avoiding extra whitespace inside double-curlies with plain variables
 #
 # When you add use of a new {{variable}}, run
 #   ./deploy.sh -Bprod
 #   ./deploy.sh -Bstaging
 #   ./deploy.sh -Bdev
 # to ensure all three paths in deploy.sh supply a value!
 # (that an "add VAR" has been added to the script)
 # this will also catch bad int/bool values, but not empty strings.
 # XXX maybe this check can be run by pre-commit??
 #
 # As a rule, "policy" decisions should be made in deploy.sh,
 # which should pass on the results as values to be subtituted
 # in this template.  The type of deployment (dev/staging/prod)
 # has explicitly NOT been passed as a value to this template
 # to discourage decision-making here!
-#}
version: "3.8"
# https://docs.docker.com/compose/compose-file/compose-file-v3/
# https://github.com/compose-spec/compose-spec/blob/master/spec.md
{#
 # The compose-file-v3 spec above says the following options are
 # ignored with "docker stack deploy" in swarm mode: cap_add,
 # cap_drop, container_name, cgroup_parent, devices, *depends_on*,
 # external_links, links, network_mode, *restart* (use
 # deploy.restart_policy), security_opt, userns_mode as well as IPv6
 # options.
 #
 # "docker stack deploy" warning about about ignoring "build:"
 # can be ignored!!
 #
 # NOTE! Specifying a (dict) key/value overwrites any previously
 # existing value under that key (ie; for keys set in a "<<:" merge)
 # which crushes sub-dicts (like environment in a service).  So each
 # family of settings (base/elasticsearch/worker) has (at least) two
 # groups of default settings: "foo-service-settings" for keys in a
 # service dict "foo-environment-vars" for environment variables
 # (declared inline under foo-service-settings)
 #}

################ common settings for all services

x-base-service-settings: &base-service-settings
  environment: &base-environment-vars
    # Log everything in GMT!!!
    TZ: "GMT"
    ELASTICSEARCH_INDEX_NAME_PREFIX: mediacloud_search_text

{% if elasticsearch_containers > 0 %}
################ Elastic Search settings

# common service settings for all elasticsearch containers
x-es-service-settings: &es-service-settings
  <<: *base-service-settings
  deploy: &es-deploy-settings
    placement:
      constraints:
        - {{elasticsearch_placement_constraint}}

  environment: &es-environment-vars
    <<: *base-environment-vars
{% if elasticsearch_containers > 1 %}
    ES_JAVA_OPTS: "-Xms20g -Xmx20g"
    bootstrap.memory_lock: "true"
    cluster.initial_master_nodes: {{elasticsearch_nodes}}
    cluster.name: {{elasticsearch_cluster}}
    discovery.seed_hosts: {{elasticsearch_nodes}}
    network.publish_host: _eth1_
    node.roles: master,data
{% else %}
    ES_JAVA_OPTS: "-Xms1g -Xmx1g"
    discovery.type: single-node
{% endif %}
    path.repo: "/var/backups/elasticsearch"
    xpack.security.enabled: "false"

  image: {{elasticsearch_image}}

  ulimits:
    # Memory lock check:allocate unlimited amount of locked memory to ES
    memlock:
      soft: -1
      hard: -1

# common volume for elasticsearch containers
x-es-backup-volume: &es-backup-volume "elasticsearch_data_backup:/var/backups/elasticsearch"
{% endif %}

################ Worker settings

# worker common service settings
x-worker-service-settings: &worker-service-settings
  <<: *base-service-settings
  deploy: &worker-deploy-settings
    placement:
      constraints:
        - {{worker_placement_constraint}}
    restart_policy:
      condition: on-failure

  environment: &worker-environment-vars
    <<: *base-environment-vars
    ELASTICSEARCH_HOSTS: {{elasticsearch_hosts}}
    LOG_LEVEL: "info"
    RABBITMQ_URL: {{rabbitmq_url}}
    SENTRY_DSN: {{sentry_dsn}}
    SENTRY_ENVIRONMENT: {{sentry_environment}}
    STATSD_REALM: {{statsd_realm}}
    STATSD_URL: {{statsd_url}}
    STORY_FACTORY: "BaseStory"
  image: {{worker_image_full}}
  volumes:
    - worker_data:/app/data/

################ news-search-{api,ui} settings

x-news-search-service-settings: &news-search-service-settings
  environment: &news-search-environment-vars
    <<: *base-environment-vars
    INDEXES: mediacloud_search_text_older,mediacloud_search_text_other,mediacloud_search_text_2021,mediacloud_search_text_2022,mediacloud_search_text_2023,mediacloud_search_text_2024
  image: {{news_search_image}}
  deploy: &news-search-deploy-settings
    placement:
      constraints:
        - {{worker_placement_constraint}}
    restart_policy:
      condition: on-failure

# end of aliases
################################################################

services:
  {{worker_image_name}}:
    build:
      context: ..
      dockerfile: docker/Dockerfile

    deploy:
      # entry for "build" only
      replicas: 0

    image: {{worker_image_full}}

  swarm-cronjob:
    <<: *base-service-settings
    deploy:
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *base-environment-vars
      LOG_JSON: "false"
      LOG_LEVEL: info
    image: crazymax/swarm-cronjob
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

{% if rabbitmq_containers > 0 %}
  rabbitmq:
    <<: *base-service-settings
    deploy:
      <<: *worker-deploy-settings
    environment:
      <<: *base-environment-vars
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 2
    # hardwire container hostname for queues
    hostname: rabbitmq
    image: rabbitmq:3.11-management-alpine
    ports:
      - "{{rabbitmq_port}}:5672"
      - "{{rabbitmq_port+10000}}:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
{% endif %}

{% if elasticsearch_containers > 0 %}
  es-setup:
    <<: *es-service-settings
    command: bash -c "chown -R elasticsearch:elasticsearch /var/backups/elasticsearch"
    deploy:
      <<: *es-deploy-settings
      # run once, on stack deploy
      restart_policy:
        condition: none
    image: {{elasticsearch_image}}
    user: root
    volumes:
      - *es-backup-volume

{% for i in range(1,elasticsearch_containers+1) %}
  elasticsearch{{i}}:
    <<: *es-service-settings
    environment:
      <<: *es-environment-vars
      node.name: elasticsearch{{i}}
    ports:
      {% set port9200mapping = elasticsearch_port_base + i - 1 -%}
      - {{port9200mapping}}:9200
      - {{port9200mapping+100}}:9300
    volumes:
      - *es-backup-volume
      - elasticsearch_data_{{"%02d" % i}}:/usr/share/elasticsearch/data
{% endfor %}
{% endif %}

  ################ pipeline workers

  configure-pipeline:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: configure-pipeline
      ARGS: "configure_and_loop"

  fetcher-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      labels:
         swarm.cronjob.enable: "{{fetcher_cronjob_enable}}"
         swarm.cronjob.schedule: "0 0 * * *"
         swarm.cronjob.replicas: {{fetcher_num_batches}}
      replicas: {{fetcher_num_batches}}
    environment:
      <<: *worker-environment-vars
      RUN: fetcher
      ARGS: "{{fetcher_options}} --num-batches={{fetcher_num_batches}} --batch-index={{ '{{.Task.Slot}}' }}"

  parser-worker:
    <<: *worker-service-settings
    # run multiple replicas?
    environment:
      <<: *worker-environment-vars
      RUN: parser
      # restrict (numpy/openblas??) threads? w/ OPENBLAS_NUM_THREADS: n??

  # PB: if running rabbitmq cluster (process on each ES node) w/ Docker
  # have numbered service instances: importer-workerN,
  # each tied to corresponding storage node?
  importer-worker:
    <<: *worker-service-settings
    # run multiple replicas? on es nodes??
    environment:
      <<: *worker-environment-vars
      RUN: importer
      ELASTICSEARCH_REPLICAS: {{elasticsearch_importer_replicas}}
      ELASTICSEARCH_SHARDS: {{elasticsearch_importer_shards}}

  archiver-worker:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: archiver
      ARCHIVER_S3_BUCKET: {{archiver_s3_bucket}}
      ARCHIVER_S3_REGION: {{archiver_s3_region}}
      ARCHIVER_S3_ACCESS_KEY_ID: {{archiver_s3_access_key_id}}
      ARCHIVER_S3_SECRET_ACCESS_KEY: {{archiver_s3_secret_access_key}}

  ################ stats reporters

  # almost certainly needs to run on a swarm manager
  docker-stats:
    <<: *worker-service-settings
    deploy: &worker-deploy-settings
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *worker-environment-vars
      RUN: docker-stats
      ARGS: "--stack {{stack_name}}"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

  # run a copy on each ES server?
  elastic-stats:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: elastic-stats

  # a QApp; needs RABBITMQ_URL, STATSD_{REALM,URL}:
  # run a copy on each RabbitMQ cluster node (for node stats & redundancy)?
  rabbitmq-stats:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: rabbitmq-stats

{% if elasticsearch_snapshot_cronjob_enable == 'true' %}
  # Elasticsearch Snaphost worker
  elastic-snapshots:
    <<: *worker-service-settings
    labels:
        swarm.cronjob.enable: "{{elasticsearch_snapshot_cronjob_enable}}"
        swarm.cronjob.schedule: "0 0 * * *"
    environment:
      <<: *worker-environment-vars
      RUN: elastic-snapshots
      ELASTICSEARCH_SNAPSHOT_REPO: mediacloud-elasticsearch-snapshots
{% endif %}

  ################ news search api (built elsewhere)

  # NOTE! Not building Docker image here (yet)
  # until version/revision control issues worked out.

  news-search-api:
    <<: *news-search-service-settings
    environment:
      <<: *news-search-environment-vars
      DESCRIPTION: "Descriptive text here"
      ESHOSTS: {{elasticsearch_hosts}}
      ESOPTS: '{"timeout": 60, "max_retries": 3}' # 'timeout' parameter is deprecated
      TERMAGGRS: top,significant,rare
      TERMFIELDS: article_title,text_content
    ports:
      - {{news_search_api_port}}:8000

  news-search-ui:
    <<: *news-search-service-settings
    command: streamlit run ui.py
    environment:
      <<: *news-search-environment-vars
      APIURL: http://news-search-api:8000/v1
      TITLE: {{news_search_ui_title}}
    ports:
      - {{news_search_ui_port}}:8501

volumes:
{% if elasticsearch_containers > 0 %}
{% for i in range(1,elasticsearch_containers+1) %}
  elasticsearch_data_{{"%02d" % i}}:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: {{volume_device_prefix}}elasticsearch_data_{{"%02d" % i}}
{%- endfor %}
  elasticsearch_data_backup:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: {{volume_device_prefix}}elasticsearch_data_backup
{% endif %}
{% if rabbitmq_containers > 0 %}
  rabbitmq_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: {{volume_device_prefix}}rabbitmq
{% endif %}
  worker_data:
    # Neccesary for diskstory, ideally not used:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: {{volume_device_prefix}}worker_data

networks:
  default:
    # order dependent?!
    driver: overlay
    attachable: true
    name: {{network_name}}
