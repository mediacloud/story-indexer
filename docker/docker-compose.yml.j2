{# docker-compose.yml.j2 -- a Jinja2 template
 # see https://jinja.palletsprojects.com/en/
 #
 # conventions:
 # services/anchors lower-case and hyphen-delimited (services are prefixed with stack_)
 # jinja variables lower-case and underscore_delimited
 # dictionary keys in ASCII sort order,
 # avoiding extra whitespace inside double-curlies with plain variables
 #
 # When you add use of a new {{variable}}, run
 #   ./deploy.sh -Bprod
 #   ./deploy.sh -Bstaging
 #   ./deploy.sh -Bdev
 # to ensure all three paths in deploy.sh supply a value!
 # (that an "add VAR" has been added to the script)
 # this will also catch bad int/bool values, but not empty strings.
 # XXX maybe this check can be run by pre-commit??
 #
 # As a rule, "policy" decisions should be made in deploy.sh,
 # which should pass on the results as values to be subtituted
 # in this template.  The type of deployment (dev/staging/prod)
 # has explicitly NOT been passed as a value to this template
 # to discourage decision-making here!
-#}
version: "3.8"
# https://docs.docker.com/compose/compose-file/compose-file-v3/
# https://github.com/compose-spec/compose-spec/blob/master/spec.md
{#
 # The compose-file-v3 spec above says the following options are
 # ignored with "docker stack deploy" in swarm mode: cap_add,
 # cap_drop, container_name, cgroup_parent, devices, *depends_on*,
 # external_links, links, network_mode, *restart* (use
 # deploy.restart_policy), security_opt, userns_mode as well as IPv6
 # options.
 #
 # "docker stack deploy" warning about about ignoring "build:"
 # can be ignored!!
 #
 # NOTE! Specifying a (dict) key/value overwrites any previously
 # existing value under that key (ie; for keys set in a "<<:" merge)
 # which crushes sub-dicts (like environment in a service).  So each
 # family of settings (base/elasticsearch/worker) has (at least) two
 # groups of default settings: "foo-service-settings" for keys in a
 # service dict "foo-environment-vars" for environment variables
 # (declared inline under foo-service-settings)
 #}

# context:
# stack_name: {{stack_name}}
# deployment_branch: {{deployment_branch}}
# deployment_date_time: {{deployment_date_time}}
# deployment_git_hash: {{deployment_git_hash}}
# deployment_host: {{deployment_host}}
# deployment_options: {{deployment_options}}
# deployment_user: {{deployment_user}}
# pipeline_type: {{pipeline_type}}

################ common settings for all services

x-base-service-settings: &base-service-settings
  environment: &base-environment-vars
    # Log everything in GMT!!!
    TZ: "GMT"
    ELASTICSEARCH_INDEX_NAME_PREFIX: mediacloud_search_text

{% if elasticsearch_containers > 0 %}
################ Elastic Search settings

# common service settings for all elasticsearch containers
x-es-service-settings: &es-service-settings
  <<: *base-service-settings
  deploy: &es-deploy-settings
    placement:
      constraints:
        - {{elasticsearch_placement_constraint}}

  environment: &es-environment-vars
    <<: *base-environment-vars
{% if elasticsearch_containers > 1 %}
    ES_JAVA_OPTS: "-Xms20g -Xmx20g"
    bootstrap.memory_lock: "true"
    cluster.initial_master_nodes: {{elasticsearch_nodes}}
    cluster.name: {{elasticsearch_cluster}}
    discovery.seed_hosts: {{elasticsearch_nodes}}
    network.publish_host: _eth1_
    node.roles: master,data
{% else %}
    ES_JAVA_OPTS: "-Xms1g -Xmx1g"
    discovery.type: single-node
{% endif %}
    path.repo: "/var/backups/elasticsearch"
    xpack.security.enabled: "false"

  image: {{elasticsearch_image}}

  ulimits:
    # Memory lock check:allocate unlimited amount of locked memory to ES
    memlock:
      soft: -1
      hard: -1

# common volume for elasticsearch containers
# (would be nice to declare default volumes list above,
# but YAML doesn't have a list merge operator (like <<: does for mappings)
# see https://github.com/yaml/yaml/issues/48)
x-es-backup-volume: &es-backup-volume "elasticsearch_data_backup:/var/backups/elasticsearch"
{% endif %}

################ Worker settings

# worker common service settings
x-worker-service-settings: &worker-service-settings
  <<: *base-service-settings
  deploy: &worker-deploy-settings
    placement:
      constraints:
        - {{worker_placement_constraint}}
    restart_policy:
      condition: on-failure

  environment: &worker-environment-vars
    <<: *base-environment-vars
    DEPLOYMENT_ID: {{deployment_id}}
    ELASTICSEARCH_HOSTS: {{elasticsearch_hosts}}
    LOG_LEVEL: "info"
    RABBITMQ_URL: {{rabbitmq_url}}
    SENTRY_DSN: {{sentry_dsn}}
    SENTRY_ENVIRONMENT: {{sentry_environment}}
    STATSD_REALM: {{statsd_realm}}
    STATSD_URL: {{statsd_url}}
    STORY_FACTORY: "BaseStory"
    SYSLOG_HOST: {{syslog_sink_container}}
    SYSLOG_PORT: 5140
  image: {{worker_image_full}}
  volumes:
    - worker_data:/app/data/

################ news-search-{api,ui} settings

x-news-search-service-settings: &news-search-service-settings
  environment: &news-search-environment-vars
    <<: *base-environment-vars
    INDEXES: mediacloud_search_text_older,mediacloud_search_text_other,mediacloud_search_text_2021,mediacloud_search_text_2022,mediacloud_search_text_2023,mediacloud_search_text_2024
  image: {{news_search_image}}
  deploy: &news-search-deploy-settings
    placement:
      constraints:
        - {{worker_placement_constraint}}
    restart_policy:
      condition: on-failure

# end of aliases
################################################################

services:
  {{worker_image_name}}:
    build:
      context: ..
      dockerfile: docker/Dockerfile

    deploy:
      # entry for "build" only
      replicas: 0

    image: {{worker_image_full}}

  {{syslog_sink_container}}:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *worker-environment-vars
      LOG_DIR: /app/data/logs
      RUN: syslog-sink
      # gets SYSLOG_PORT via worker-environment-vars

  swarm-cronjob:
    <<: *base-service-settings
    deploy:
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *base-environment-vars
      LOG_JSON: "false"
      LOG_LEVEL: info
    image: crazymax/swarm-cronjob
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

{% if rabbitmq_containers > 0 %}
  rabbitmq:
    <<: *base-service-settings
    deploy:
      <<: *worker-deploy-settings
    environment:
      <<: *base-environment-vars
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 2
    # hardwire container hostname for queues
    hostname: rabbitmq
    image: rabbitmq:3.11-management-alpine
    ports:
      - "{{rabbitmq_port_exported}}:{{rabbitmq_port}}"
      - "{{rabbitmq_port_exported+10000}}:{{rabbitmq_port+10000}}"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
{% endif %}

{% if elasticsearch_containers > 0 %}
  es-setup:
    <<: *es-service-settings
    command: bash -c "chown -R elasticsearch:elasticsearch /var/backups/elasticsearch"
    deploy:
      <<: *es-deploy-settings
      # run once, on stack deploy
      restart_policy:
        condition: none
    image: {{elasticsearch_image}}
    user: root
    volumes:
      - *es-backup-volume

{% for i in range(1,elasticsearch_containers+1) %}
  elasticsearch{{i}}:
    <<: *es-service-settings
    environment:
      <<: *es-environment-vars
      node.name: elasticsearch{{i}}
    ports:
      {% set port9200mapping = elasticsearch_port_base_exported + i - 1 -%}
      - {{port9200mapping}}:{{elasticsearch_port_base}}
      - {{port9200mapping+100}}:{{elasticsearch_port_base+100}}
    volumes:
      - *es-backup-volume
      - elasticsearch_data_{{"%02d" % i}}:/usr/share/elasticsearch/data
{% endfor %}
{% endif %}

  ################ pipeline workers

  configure-pipeline:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: configure-pipeline
      ARGS: "--type {{pipeline_type}} configure_and_loop"

{% if queuer_type != '' %}
  {{queuer_type}}:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      labels:
         swarm.cronjob.enable: "{{fetcher_cronjob_enable}}"
         swarm.cronjob.schedule: "45 0 * * *"
      replicas: 1
    environment:
      <<: *worker-environment-vars
      RUN: {{queuer_type}}
      ARGS: "{{queuer_args}}"
      QUEUER_S3_ACCESS_KEY_ID: {{queuer_s3_access_key_id}}
      QUEUER_S3_REGION: {{queuer_s3_region}}
      QUEUER_S3_SECRET_ACCESS_KEY: {{queuer_s3_secret_access_key}}

{% endif %}

{% if pipeline_type == 'batch-fetcher' %}
  # scrapy batch-based fetcher:
  fetcher-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      labels:
         swarm.cronjob.enable: "{{fetcher_cronjob_enable}}"
         swarm.cronjob.schedule: "0 0 * * *"
         swarm.cronjob.replicas: {{fetcher_replicas}}
      replicas: {{fetcher_replicas}}
    environment:
      <<: *worker-environment-vars
      RUN: fetcher
      ARGS: "{{fetcher_options}} --num-batches={{fetcher_replicas}} --batch-index={{ '{{.Task.Slot}}' }}"
{% elif pipeline_type == 'historical' %}
  # fetches historical articles from S3, using Stories queued by hist-queuer:
  hist-fetcher:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      replicas: {{fetcher_replicas}}
    environment:
      <<: *worker-environment-vars
      RUN: hist-fetcher
      ARCHIVER_S3_REGION: {{archiver_s3_region}}
      ARCHIVER_S3_ACCESS_KEY_ID: {{archiver_s3_access_key_id}}
      ARCHIVER_S3_SECRET_ACCESS_KEY: {{archiver_s3_secret_access_key}}
{% elif pipeline_type == 'queue-fetcher' %}
  # queue-based HTTP fetcher, using Stories queued by rss-queuer:
  queue-fetcher:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
    environment:
      <<: *worker-environment-vars
      RUN: queue-fetcher
      # pass ARGS: -Wn based on fetcher_replicas??
{% elif pipeline_type == 'archive' %}
   # No fetcher for loading archives (yet, anyway).  Hopefully each
   # archive has enough stories to keep pipeline busy enough not to
   # need parallel fetching.
{% endif %}

  parser-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      replicas: {{parser_replicas}}
    environment:
      <<: *worker-environment-vars
      # Run py3lang/numpy/openblas dot operator single threaded:
      OPENBLAS_NUM_THREADS: 1
      RUN: parser

  # Have at least one importer-worker running on each storage node
  # (connecting only to the local ES process)?
  # If RabbitMQ cluster running (on each storage node)
  # have the importer connect only to the local RabbitMQ??
  importer-worker:
    <<: *worker-service-settings
    # run multiple replicas? on es nodes??
    environment:
      <<: *worker-environment-vars
      RUN: importer
      ARGS: {{importer_args}}
      ELASTICSEARCH_REPLICAS: {{elasticsearch_importer_replicas}}
      ELASTICSEARCH_SHARDS: {{elasticsearch_importer_shards}}

  archiver-worker:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: archiver
      ARCHIVER_PREFIX: {{archiver_prefix}}
      ARCHIVER_S3_BUCKET: {{archiver_s3_bucket}}
      ARCHIVER_S3_REGION: {{archiver_s3_region}}
      ARCHIVER_S3_ACCESS_KEY_ID: {{archiver_s3_access_key_id}}
      ARCHIVER_S3_SECRET_ACCESS_KEY: {{archiver_s3_secret_access_key}}

  ################ stats reporters

  # almost certainly needs to run on a swarm manager
  docker-stats:
    <<: *worker-service-settings
    deploy: &worker-deploy-settings
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *worker-environment-vars
      RUN: docker-stats
      ARGS: "--stack {{stack_name}}"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

  # run a copy on each ES server?
  elastic-stats:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: elastic-stats

  # a QApp; needs RABBITMQ_URL, STATSD_{REALM,URL}:
  # run a copy on each RabbitMQ cluster node (for node stats & redundancy)?
  rabbitmq-stats:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: rabbitmq-stats

{% if elasticsearch_snapshot_cronjob_enable == 'true' %}
  # Elasticsearch Snaphost worker
  elastic-snapshots:
    <<: *worker-service-settings
    labels:
        swarm.cronjob.enable: "{{elasticsearch_snapshot_cronjob_enable}}"
        swarm.cronjob.schedule: "0 0 * * *"
    environment:
      <<: *worker-environment-vars
      RUN: elastic-snapshots
      ELASTICSEARCH_SNAPSHOT_REPO: mediacloud-elasticsearch-snapshots
{% endif %}

  ################ news search api (built elsewhere)

  # NOTE! Not building Docker image here (yet)
  # until version/revision control issues worked out.

  news-search-api:
    <<: *news-search-service-settings
    environment:
      <<: *news-search-environment-vars
      DESCRIPTION: "Descriptive text here"
      ESHOSTS: {{elasticsearch_hosts}}
      ESOPTS: '{"timeout": 60, "max_retries": 3}' # 'timeout' parameter is deprecated
      TERMAGGRS: top,significant,rare
      TERMFIELDS: article_title,text_content
    ports:
      - {{news_search_api_port_exported}}:{{news_search_api_port}}

  news-search-ui:
    <<: *news-search-service-settings
    command: streamlit run ui.py
    environment:
      <<: *news-search-environment-vars
      APIURL: http://news-search-api:8000/v1
      TITLE: {{news_search_ui_title}}
    ports:
      - {{news_search_ui_port_exported}}:{{news_search_ui_port}}

{% macro define_volume(prefix, suffix='') %}
  {{prefix}}{{suffix}}:
{%- if volume_device_prefix %}
    driver: local
    driver_opts:
      type: none
      o: bind
      device: {{volume_device_prefix}}{{prefix}}{{suffix}}
{% endif -%}
{% endmacro -%}

volumes:
{% if elasticsearch_containers > 0 -%}
{% for i in range(1,elasticsearch_containers+1) %}
{{ define_volume('elasticsearch_data',"_%02d" % i) }}
{%- endfor %}
{{ define_volume('elasticsearch_data_backup') }}
{%- endif %}
{% if rabbitmq_containers > 0 %}
{{ define_volume('rabbitmq_data') }}
{% endif %}
{{ define_volume('worker_data') }}

networks:
  default:
    # order dependent?!
    driver: overlay
    attachable: true
    name: {{network_name}}
