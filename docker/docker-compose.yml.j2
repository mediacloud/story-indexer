{# docker-compose.yml.j2 -- a Jinja2 template
 # see https://jinja.palletsprojects.com/en/
 #
 # conventions:
 # services/anchors lower-case and hyphen-delimited (services are prefixed with stack_)
 # jinja variables lower-case and underscore_delimited
 # dictionary keys in ASCII sort order,
 # avoiding extra whitespace inside double-curlies with plain variables
 #
 # When you add use of a new {{variable}}, run
 #   ./deploy.sh -Bprod
 #   ./deploy.sh -Bstaging
 #   ./deploy.sh -Bdev
 # to ensure all three paths in deploy.sh supply a value!
 # (that an "add VAR" has been added to the script)
 # this will also catch bad int/bool values, but not empty strings.
 # XXX maybe this check can be run by pre-commit??
 #
 # As a rule, "policy" decisions should be made in deploy.sh,
 # which should pass on the results as values to be subtituted
 # in this template.  The type of deployment (dev/staging/prod)
 # has explicitly NOT been passed as a value to this template
 # to discourage decision-making here!
-#}
# https://docs.docker.com/compose/compose-file/compose-file-v3/
# https://github.com/compose-spec/compose-spec/blob/master/spec.md
# removed version tag to silence deprecation messages.
{#
 # The compose-file-v3 spec above says the following options are
 # ignored with "docker stack deploy" in swarm mode: cap_add,
 # cap_drop, container_name, cgroup_parent, devices, *depends_on*,
 # external_links, links, network_mode, *restart* (use
 # deploy.restart_policy), security_opt, userns_mode as well as IPv6
 # options.
 #
 # "docker stack deploy" warning about about ignoring "build:"
 # can be ignored!!
 #
 # NOTE! Specifying a (dict) key/value overwrites any previously
 # existing value under that key (ie; for keys set in a "<<:" merge)
 # which crushes sub-dicts (like environment in a service).  So each
 # family of settings (base/elasticsearch/worker) has (at least) two
 # groups of default settings: "foo-service-settings" for keys in a
 # service dict "foo-environment-vars" for environment variables
 # (declared inline under foo-service-settings)
 #}

# context:
# stack_name: {{stack_name}}
# deployment_branch: {{deployment_branch}}
# deployment_date_time: {{deployment_date_time}}
# deployment_git_hash: {{deployment_git_hash}}
# deployment_host: {{deployment_host}}
# deployment_options: {{deployment_options}}
# deployment_user: {{deployment_user}}
# pipeline_type: {{pipeline_type}}

################ common settings for all services

x-base-service-settings: &base-service-settings
  environment: &base-environment-vars
    # Log everything in GMT!!!
    TZ: "GMT"

{% if elasticsearch_containers > 0 %}
################ Elastic Search settings

# common service settings for all elasticsearch containers
x-es-service-settings: &es-service-settings
  <<: *base-service-settings
  deploy: &es-deploy-settings
    placement:
      constraints:
        - {{elasticsearch_placement_constraint}}

  environment: &es-environment-vars
    <<: *base-environment-vars
{% if elasticsearch_containers > 1 %}
    ES_JAVA_OPTS: "-Xms20g -Xmx20g"
    bootstrap.memory_lock: "true"
    cluster.initial_master_nodes: {{elasticsearch_nodes}}
    cluster.name: {{elasticsearch_cluster}}
    discovery.seed_hosts: {{elasticsearch_nodes}}
    network.publish_host: _eth1_
    node.roles: master,data_content, data_hot
{% else %}
    ES_JAVA_OPTS: "-Xms1g -Xmx1g"
    discovery.type: single-node
{% endif %}
    path.repo: "/var/backups/elasticsearch"
    xpack.security.enabled: "false"

  image: {{elasticsearch_image}}

  ulimits:
    # Memory lock check:allocate unlimited amount of locked memory to ES
    memlock:
      soft: -1
      hard: -1

# common volume for elasticsearch containers
# (would be nice to declare default volumes list above,
# but YAML doesn't have a list merge operator (like <<: does for mappings)
# see https://github.com/yaml/yaml/issues/48)
x-es-backup-volume: &es-backup-volume "elasticsearch_data_backup:/var/backups/elasticsearch"
{% endif %}

################ Worker settings

# worker common service settings
x-worker-service-settings: &worker-service-settings
  <<: *base-service-settings
  deploy: &worker-deploy-settings
    placement:
      constraints:
        - {{worker_placement_constraint}}
    restart_policy:
      condition: on-failure

  environment: &worker-environment-vars
    <<: *base-environment-vars
    DEPLOYMENT_ID: {{deployment_id}}
    ELASTICSEARCH_HOSTS: {{elasticsearch_hosts}}
    ELASTICSEARCH_CONFIG_DIR: {{elasticsearch_config_dir}}
    LOG_LEVEL: "info"
    RABBITMQ_URL: {{rabbitmq_url}}
    SENTRY_DSN: {{sentry_dsn}}
    SENTRY_ENVIRONMENT: {{sentry_environment}}
    STATSD_REALM: {{statsd_realm}}
    STATSD_URL: {{statsd_url}}
    STORY_FACTORY: "BaseStory"
    SYSLOG_HOST: {{syslog_sink_container}}
    SYSLOG_PORT: 5140
  image: {{worker_image_full}}
  volumes:
    - worker_data:/app/data/

# end of aliases
################################################################

services:
  {{worker_image_name}}:
    build:
      context: ..
      dockerfile: docker/Dockerfile

    deploy:
      # entry for "build" only
      replicas: 0

    image: {{worker_image_full}}

  {{syslog_sink_container}}:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *worker-environment-vars
      LOG_DIR: /app/data/logs
      RUN: syslog-sink
      # gets SYSLOG_PORT via worker-environment-vars

  swarm-cronjob:
    <<: *base-service-settings
    deploy:
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *base-environment-vars
      LOG_JSON: "false"
      LOG_LEVEL: info
    image: crazymax/swarm-cronjob
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

{% if rabbitmq_containers > 0 %}
  rabbitmq:
    <<: *base-service-settings
    deploy:
      <<: *worker-deploy-settings
    environment:
      <<: *base-environment-vars
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 2
    # hardwire container hostname for queues
    hostname: rabbitmq
    image: rabbitmq:3.11-management-alpine
    ports:
      - "{{rabbitmq_port_exported}}:{{rabbitmq_port}}"
      - "{{rabbitmq_port_exported+10000}}:{{rabbitmq_port+10000}}"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
{% endif %}

{% if elasticsearch_containers > 0 %}
  es-setup:
    <<: *es-service-settings
    command: bash -c "chown -R elasticsearch:elasticsearch /var/backups/elasticsearch"
    deploy:
      <<: *es-deploy-settings
      # run once, on stack deploy
      restart_policy:
        condition: none
    image: {{elasticsearch_image}}
    user: root
    volumes:
      - *es-backup-volume

{% for i in range(1,elasticsearch_containers+1) %}
  elasticsearch{{i}}:
    <<: *es-service-settings
    environment:
      <<: *es-environment-vars
      node.name: elasticsearch{{i}}
    ports:
      {% set port9200mapping = elasticsearch_port_base_exported + i - 1 -%}
      - {{port9200mapping}}:{{elasticsearch_port_base}}
      - {{port9200mapping+100}}:{{elasticsearch_port_base+100}}
    volumes:
      - *es-backup-volume
      - elasticsearch_data_{{"%02d" % i}}:/usr/share/elasticsearch/data
{% endfor %}
{% endif %}

  ################ pipeline workers

  configure-pipeline:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: configure-pipeline
      ARGS: "--type {{pipeline_type}} configure_and_loop"

{% if queuer_type != '' %}
{# hist-queuer, arch-queuer, rss-queuer
 # started every 5 minutes to check for low output queue (<100K stories)
 # If queue is consumed faster than 333 stories/second, queue can run dry.

 # NOTE: skip-running: "true" means won't interrupt a running
 # process.  Necessary because a single arch-queuer couldn't build a
 # backlog, and runs continuously. For hist-queuer on bernstein for
 # 2023, average queue rate was about 630 stories/second, logging and
 # counting every story so a 400K to 600K story one-day CSV file took
 # over 10 minutes.

 # The Queuer base class used by {arch,hist,rss}-queuer scripts has a
 # --loop option to run continuously (sleeping one minute between
 # checks), which would work for arch- and hist- queuers (since the
 # list of input files is static), but not for rss-queuer, which
 # needs to check for newly written files.  BUT it was too much
 # bother to have different policies/options for the different
 # queuers, AND my hope is to replace rss-queuer with a process that
 # polls the rss-fetcher via its' API for batches of N (1000?)
 # stories, queuing them and then saving a bookmark for the next
 # query.
 #}

  {{queuer_type}}:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      labels:
         swarm.cronjob.enable: "{{queuer_cronjob_enable}}"
         swarm.cronjob.schedule: "0 {{queuer_cronjob_minutes}} * * * *" # sec min hr DOM mon DOW
         swarm.cronjob.replicas: {{queuer_cronjob_replicas}}
         swarm.cronjob.skip-running: "true" {# don't restart if currently running #}
      replicas: {{queuer_initial_replicas}}
    environment:
      <<: *worker-environment-vars
      RUN: {{queuer_type}}
      ARGS: "{{queuer_args}}"
      ARCHIVER_B2_BUCKET: {{archiver_b2_bucket}}
      ARCHIVER_B2_REGION: {{archiver_b2_region}}
      ARCHIVER_B2_ACCESS_KEY_ID: {{archiver_b2_access_key_id}}
      ARCHIVER_B2_SECRET_ACCESS_KEY: {{archiver_b2_secret_access_key}}
      QUEUER_S3_ACCESS_KEY_ID: {{queuer_s3_access_key_id}}
      QUEUER_S3_REGION: {{queuer_s3_region}}
      QUEUER_S3_SECRET_ACCESS_KEY: {{queuer_s3_secret_access_key}}
      RSS_FETCHER_PASS: {{rss_fetcher_pass}}
      RSS_FETCHER_URL: {{rss_fetcher_url}}
      RSS_FETCHER_USER: {{rss_fetcher_user}}

{% endif %}
{% if pipeline_type == 'batch-fetcher' %}
  # scrapy batch-based fetcher:
  fetcher-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      labels:
         swarm.cronjob.enable: "{{fetcher_cronjob_enable}}"
         swarm.cronjob.schedule: "0 0 * * *"
         swarm.cronjob.replicas: {{fetcher_num_batches}}
      replicas: {{fetcher_num_batches}}
    environment:
      <<: *worker-environment-vars
      RUN: fetcher
      ARGS: "{{fetcher_options}} --num-batches={{fetcher_num_batches}} --batch-index={{ '{{.Task.Slot}}' }}"
{% elif pipeline_type == 'historical' %}
  # fetches historical articles from S3, using Stories queued by hist-queuer:
  hist-fetcher:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      replicas: {{hist_fetcher_replicas}}
    environment:
      <<: *worker-environment-vars
      RUN: hist-fetcher
      QUEUER_S3_ACCESS_KEY_ID: {{queuer_s3_access_key_id}}
      QUEUER_S3_REGION: {{queuer_s3_region}}
      QUEUER_S3_SECRET_ACCESS_KEY: {{queuer_s3_secret_access_key}}
{% elif pipeline_type == 'queue-fetcher' or pipeline_type == 'csv' %}
  # queue-based HTTP fetcher, using Stories queued by rss-queuer:
  queue-fetcher:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
    environment:
      <<: *worker-environment-vars
      RUN: queue-fetcher
      # pass ARGS: -Wn based on fetcher_replicas??
{% elif pipeline_type == 'archive' %}
   # No fetcher for loading archives (yet, anyway).
   # arch-queuer reads archives directly and queues stories.
   # Hopefully, each archive has enough stories to keep
   # pipeline busy enough not to need parallel fetching.
{% endif %}

  parser-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      replicas: {{parser_replicas}}
    environment:
      <<: *worker-environment-vars
      # Run py3lang/numpy/openblas dot operator single threaded:
      OPENBLAS_NUM_THREADS: 1
      RUN: parser

  # Have at least one importer-worker running on each storage node
  # (connecting only to the local ES process)?
  # If RabbitMQ cluster running (on each storage node)
  # have the importer connect only to the local RabbitMQ??
  importer-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      # run on each ES node?
      replicas: {{importer_replicas}}
    environment:
      <<: *worker-environment-vars
      RUN: importer
      ARGS: {{importer_args}}

  archiver-worker:
    <<: *worker-service-settings
    deploy:
      <<: *worker-deploy-settings
      replicas: {{archiver_replicas}}
    environment:
      <<: *worker-environment-vars
      RUN: archiver
      ARCHIVER_B2_BUCKET: {{archiver_b2_bucket}}
      ARCHIVER_B2_REGION: {{archiver_b2_region}}
      ARCHIVER_B2_ACCESS_KEY_ID: {{archiver_b2_access_key_id}}
      ARCHIVER_B2_SECRET_ACCESS_KEY: {{archiver_b2_secret_access_key}}
      ARCHIVER_PREFIX: {{archiver_prefix}}
      ARCHIVER_REMOVE_LOCAL: "non-empty-to-remove-after-upload"
      ARCHIVER_S3_BUCKET: {{archiver_s3_bucket}}
      ARCHIVER_S3_REGION: {{archiver_s3_region}}
      ARCHIVER_S3_ACCESS_KEY_ID: {{archiver_s3_access_key_id}}
      ARCHIVER_S3_SECRET_ACCESS_KEY: {{archiver_s3_secret_access_key}}

  ################ stats reporters

  # almost certainly needs to run on a swarm manager
  docker-stats:
    <<: *worker-service-settings
    deploy: &worker-deploy-settings
      placement:
        constraints:
          - node.role == manager
    environment:
      <<: *worker-environment-vars
      RUN: docker-stats
      ARGS: "--stack {{stack_name}}"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"

  # run a copy on each ES server?
  elastic-stats:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: elastic-stats

  # a QApp; needs RABBITMQ_URL, STATSD_{REALM,URL}:
  # run a copy on each RabbitMQ cluster node (for node stats & redundancy)?
  rabbitmq-stats:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      RUN: rabbitmq-stats

{% if elasticsearch_snapshot_cronjob_enable == 'true' %}
  # Elasticsearch Snaphost worker
  elastic-snapshots:
    <<: *worker-service-settings
    labels:
        swarm.cronjob.enable: "{{elasticsearch_snapshot_cronjob_enable}}"
        swarm.cronjob.schedule: "0 0 * * *"
    environment:
      <<: *worker-environment-vars
      RUN: elastic-snapshots
      ELASTICSEARCH_SNAPSHOT_REPO: mediacloud-elasticsearch-snapshots
{% endif %}

  ################ news search api (built elsewhere)

  # NOTE! Not building Docker image here (yet)
  # until version/revision control issues worked out.

  # run to apply ILM ES
  elastic-conf:
    <<: *worker-service-settings
    environment:
      <<: *worker-environment-vars
      ELASTICSEARCH_ILM_MAX_AGE: {{elasticsearch_ilm_max_age}}
      ELASTICSEARCH_ILM_MAX_SHARD_SIZE: {{elasticsearch_ilm_max_shard_size}}
      ELASTICSEARCH_SHARD_COUNT : {{elasticsearch_shard_count}}
      ELASTICSEARCH_SHARD_REPLICAS: {{elasticsearch_shard_replicas}}
      ELASTICSEARCH_SNAPSHOT_REPO: {{elasticsearch_snapshot_repo}}
      ELASTICSEARCH_SNAPSHOT_REPO_SETTINGS_BUCKET: {{elasticsearch_snapshot_repo_settings_bucket}}
      ELASTICSEARCH_SNAPSHOT_REPO_SETTINGS_ENDPOINT: {{elasticsearch_snapshot_repo_settings_endpoint}}
      ELASTICSEARCH_SNAPSHOT_REPO_SETTINGS_LOCATION: {{elasticsearch_snapshot_repo_settings_location}}
      ELASTICSEARCH_SNAPSHOT_REPO_TYPE: {{elasticsearch_snapshot_repo_type}}
      ELASTICSEARCH_MAX_WAIT_TIME: {{elasticsearch_max_wait_time}}
      ELASTICSEARCH_WAIT_INTERVAL: {{elasticsearch_wait_interval}}

      RUN: elastic-conf

{% macro define_volume(prefix, suffix='') %}
  {{prefix}}{{suffix}}:
{%- if volume_device_prefix %}
    driver: local
    driver_opts:
      type: none
      o: bind
      device: {{volume_device_prefix}}{{prefix}}{{suffix}}
{% endif -%}
{% endmacro -%}

volumes:
{% if elasticsearch_containers > 0 -%}
{% for i in range(1,elasticsearch_containers+1) %}
{{ define_volume('elasticsearch_data',"_%02d" % i) }}
{%- endfor %}
{{ define_volume('elasticsearch_data_backup') }}
{%- endif %}
{% if rabbitmq_containers > 0 %}
{{ define_volume('rabbitmq_data') }}
{% endif %}
{{ define_volume('worker_data') }}

networks:
  default:
    # order dependent?!
    driver: overlay
    attachable: true
    name: {{network_name}}
